{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Image Coloration using pix2pix algorithm"
      ],
      "metadata": {
        "id": "31hnYhy91YnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemented Image coloration model in the paper \"Image-to-Image Translation with Conditional Adversarial Networks\" authored by Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros with some changes."
      ],
      "metadata": {
        "id": "AxjY37ZW13rw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfxYE0c_M5-9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading MS coco validation dataset 2017 for training the model"
      ],
      "metadata": {
        "id": "8xRyzr2KxpsT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7zVw6V6LI-4"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  DATA_DIR = tf.keras.utils.get_file(\n",
        "    \"coco.zip\",\n",
        "    \"http://images.cocodataset.org/zips/val2017.zip\",\n",
        "    extract=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgAsAHJuR4S6"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  print(DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AL82RiNSQZ9"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        " !unzip \"/root/.keras/datasets/coco.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA_bIx74oE7a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg2tIZf3TTeV"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  import os\n",
        "  import glob\n",
        "  import time\n",
        "  import numpy as np\n",
        "  from PIL import Image\n",
        "  from pathlib import Path\n",
        "  from tqdm.notebook import tqdm\n",
        "  import matplotlib.pyplot as plt\n",
        "  from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "  import torch\n",
        "  from torch import nn, optim\n",
        "  from torchvision import transforms\n",
        "  from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbkrSX4GXSF2"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  paths = glob.glob(\"/content/val2017/*.jpg\")\n",
        "  print(paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have borrowed the below codes from the above mentioned repository, but I have changed the seed value, took a smaller dataset of 5000 images instead 8000 images and I have changed the parameters to (lr_G=1e-4, lr_D=1e-4, beta1=0.40, beta2=0.9, lambda_L1=90) from (lr_G=2e-4, lr_D=2e-4, beta1=0.5, beta2=0.999, lambda_L1=100) in MainModel class __init__ function, also I have set the total number of epochs to 60 instead of 100 in mentioned github repo and saved the model weights using model_state_dict ."
      ],
      "metadata": {
        "id": "mCo31Isdx1el"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b39BIiyBXfV1"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  np.random.seed(1234)\n",
        "  paths_subset = np.random.choice(paths, 5000, replace=False)\n",
        "  rand_idxs = np.random.permutation(5000)\n",
        "  train_idxs = rand_idxs[:4500]\n",
        "  val_idxs = rand_idxs[4500:]\n",
        "  train_paths = paths_subset[train_idxs]\n",
        "  val_paths = paths_subset[val_idxs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-rZ2oDsYjXe"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  SIZE = 256\n",
        "  class ColorizationDataset(Dataset):\n",
        "    def __init__(self, paths, split='train'):\n",
        "      if split == 'train':\n",
        "        self.transforms = transforms.Compose([transforms.Resize((SIZE,SIZE), Image.BICUBIC)])\n",
        "      elif split == 'val':\n",
        "        self.transforms = transforms.Resize((SIZE,SIZE), Image.BICUBIC)\n",
        "      self.split = split\n",
        "      self.size = SIZE\n",
        "      self.paths = paths\n",
        "    def __getitem__(self, idx):\n",
        "      img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "      img = self.transforms(img)\n",
        "      img = np.array(img)\n",
        "      img_lab = rgb2lab(img).astype(\"float32\")\n",
        "      img_lab = transforms.ToTensor()(img_lab)\n",
        "      l = img_lab[[0], ...]/50. - 1.\n",
        "      ab = img_lab[[1,2], ...]/110.\n",
        "      return {'L':l, 'ab':ab}\n",
        "    def __len__(self):\n",
        "      return len(self.paths)\n",
        "  def make_dataloaders(batch_size=16, n_workers=2, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n",
        "      dataset = ColorizationDataset(**kwargs)\n",
        "      dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "      return dataloader\n",
        "\n",
        "  train_dl = make_dataloaders(paths=train_paths, split='train')\n",
        "  val_dl = make_dataloaders(paths=val_paths, split='val')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEcx6JUFc5jR"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  class UnetBlock(nn.Module):\n",
        "      def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
        "                  innermost=False, outermost=False):\n",
        "          super().__init__()\n",
        "          self.outermost = outermost\n",
        "          if input_c is None: input_c = nf\n",
        "          downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n",
        "                              stride=2, padding=1, bias=False)\n",
        "          downrelu = nn.LeakyReLU(0.2, True)\n",
        "          downnorm = nn.BatchNorm2d(ni)\n",
        "          uprelu = nn.ReLU(True)\n",
        "          upnorm = nn.BatchNorm2d(nf)\n",
        "          \n",
        "          if outermost:\n",
        "              upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
        "                                          stride=2, padding=1)\n",
        "              down = [downconv]\n",
        "              up = [uprelu, upconv, nn.Tanh()]\n",
        "              model = down + [submodule] + up\n",
        "          elif innermost:\n",
        "              upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n",
        "                                          stride=2, padding=1, bias=False)\n",
        "              down = [downrelu, downconv]\n",
        "              up = [uprelu, upconv, upnorm]\n",
        "              model = down + up\n",
        "          else:\n",
        "              upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
        "                                          stride=2, padding=1, bias=False)\n",
        "              down = [downrelu, downconv, downnorm]\n",
        "              up = [uprelu, upconv, upnorm]\n",
        "              if dropout: up += [nn.Dropout(0.5)]\n",
        "              model = down + [submodule] + up\n",
        "          self.model = nn.Sequential(*model)\n",
        "      \n",
        "      def forward(self, x):\n",
        "          if self.outermost:\n",
        "              return self.model(x)\n",
        "          else:\n",
        "              return torch.cat([x, self.model(x)], 1)\n",
        "\n",
        "  class Unet(nn.Module):\n",
        "      def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
        "          super().__init__()\n",
        "          unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
        "          for _ in range(n_down - 5):\n",
        "              unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
        "          out_filters = num_filters * 8\n",
        "          for _ in range(3):\n",
        "              unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
        "              out_filters //= 2\n",
        "          self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
        "      \n",
        "      def forward(self, x):\n",
        "          return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEvROEHFfwNH"
      },
      "outputs": [],
      "source": [
        "def init_weights(net, init='norm', gain=0.02):\n",
        "    \n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
        "            if init == 'norm':\n",
        "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
        "            elif init == 'xavier':\n",
        "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init == 'kaiming':\n",
        "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            \n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "        elif 'BatchNorm2d' in classname:\n",
        "            nn.init.normal_(m.weight.data, 1., gain)\n",
        "            nn.init.constant_(m.bias.data, 0.)\n",
        "            \n",
        "    net.apply(init_func)\n",
        "    print(f\"model initialized with {init} initialization\")\n",
        "    return net\n",
        "\n",
        "def init_model(model, device):\n",
        "    model = model.to(device)\n",
        "    model = init_weights(model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT8Z1yRNetAq"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  class PatchDiscriminator(nn.Module):\n",
        "      def __init__(self, input_c, num_filters=64, n_down=3):\n",
        "          super().__init__()\n",
        "          model = [self.get_layers(input_c, num_filters, norm=False)]\n",
        "          model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2) \n",
        "                            for i in range(n_down)]\n",
        "          model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)]\n",
        "          self.model = nn.Sequential(*model)                                                   \n",
        "          \n",
        "      def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): \n",
        "          layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          \n",
        "          if norm: layers += [nn.BatchNorm2d(nf)]\n",
        "          if act: layers += [nn.LeakyReLU(0.2, True)]\n",
        "          return nn.Sequential(*layers)\n",
        "      \n",
        "      def forward(self, x):\n",
        "          return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdjiQ0UtezIu"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  class GANLoss(nn.Module):\n",
        "      def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
        "          super().__init__()\n",
        "          self.register_buffer('real_label', torch.tensor(real_label))\n",
        "          self.register_buffer('fake_label', torch.tensor(fake_label))\n",
        "          if gan_mode == 'vanilla':\n",
        "              self.loss = nn.BCEWithLogitsLoss()\n",
        "          elif gan_mode == 'lsgan':\n",
        "              self.loss = nn.MSELoss()\n",
        "      \n",
        "      def get_labels(self, preds, target_is_real):\n",
        "          if target_is_real:\n",
        "              labels = self.real_label\n",
        "          else:\n",
        "              labels = self.fake_label\n",
        "          return labels.expand_as(preds)\n",
        "      \n",
        "      def __call__(self, preds, target_is_real):\n",
        "          labels = self.get_labels(preds, target_is_real)\n",
        "          loss = self.loss(preds, labels)\n",
        "          return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGqleyN5e8jq"
      },
      "outputs": [],
      "source": [
        "class MainModel(nn.Module):\n",
        "    def __init__(self, net_G=None, lr_G=1e-4, lr_D=1e-4, \n",
        "                 beta1=0.40, beta2=0.9, lambda_L1=90.):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.lambda_L1 = lambda_L1\n",
        "        \n",
        "        if net_G is None:\n",
        "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
        "        else:\n",
        "            self.net_G = net_G.to(self.device)\n",
        "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
        "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
        "        self.L1criterion = nn.L1Loss()\n",
        "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
        "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
        "    \n",
        "    def set_requires_grad(self, model, requires_grad=True):\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = requires_grad\n",
        "        \n",
        "    def setup_input(self, data):\n",
        "        self.L = data['L'].to(self.device)\n",
        "        self.ab = data['ab'].to(self.device)\n",
        "        \n",
        "    def forward(self):\n",
        "        self.fake_color = self.net_G(self.L)\n",
        "    \n",
        "    def backward_D(self):\n",
        "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
        "        fake_preds = self.net_D(fake_image.detach())\n",
        "        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
        "        real_image = torch.cat([self.L, self.ab], dim=1)\n",
        "        real_preds = self.net_D(real_image)\n",
        "        self.loss_D_real = self.GANcriterion(real_preds, True)\n",
        "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
        "        self.loss_D.backward()\n",
        "    \n",
        "    def backward_G(self):\n",
        "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
        "        fake_preds = self.net_D(fake_image)\n",
        "        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
        "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n",
        "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
        "        self.loss_G.backward()\n",
        "    \n",
        "    def optimize(self):\n",
        "        self.forward()\n",
        "        self.net_D.train()\n",
        "        self.set_requires_grad(self.net_D, True)\n",
        "        self.opt_D.zero_grad()\n",
        "        self.backward_D()\n",
        "        self.opt_D.step()\n",
        "        \n",
        "        self.net_G.train()\n",
        "        self.set_requires_grad(self.net_D, False)\n",
        "        self.opt_G.zero_grad()\n",
        "        self.backward_G()\n",
        "        self.opt_G.step()\n",
        "\n",
        "model = MainModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzdX6gEsfFpL"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        self.count, self.avg, self.sum = [0.] * 3\n",
        "    \n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += count * val\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "  def create_loss_meters():\n",
        "      loss_D_fake = AverageMeter()\n",
        "      loss_D_real = AverageMeter()\n",
        "      loss_D = AverageMeter()\n",
        "      loss_G_GAN = AverageMeter()\n",
        "      loss_G_L1 = AverageMeter()\n",
        "      loss_G = AverageMeter()\n",
        "      \n",
        "      return {'loss_D_fake': loss_D_fake,\n",
        "              'loss_D_real': loss_D_real,\n",
        "              'loss_D': loss_D,\n",
        "              'loss_G_GAN': loss_G_GAN,\n",
        "              'loss_G_L1': loss_G_L1,\n",
        "              'loss_G': loss_G}\n",
        "\n",
        "  def update_losses(model, loss_meter_dict, count):\n",
        "      for loss_name, loss_meter in loss_meter_dict.items():\n",
        "          loss = getattr(model, loss_name)\n",
        "          loss_meter.update(loss.item(), count=count)\n",
        "\n",
        "  def lab_to_rgb(L, ab):\n",
        "      \"\"\"\n",
        "      Takes a batch of images\n",
        "      \"\"\"\n",
        "      \n",
        "      L = (L + 1.) * 50.\n",
        "      ab = ab * 110.\n",
        "      Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "      rgb_imgs = []\n",
        "      for img in Lab:\n",
        "          img_rgb = lab2rgb(img)\n",
        "          rgb_imgs.append(img_rgb)\n",
        "      return np.stack(rgb_imgs, axis=0)\n",
        "      \n",
        "  def visualize(model, data, save=True):\n",
        "      model.net_G.eval()\n",
        "      with torch.no_grad():\n",
        "          model.setup_input(data)\n",
        "          model.forward()\n",
        "      model.net_G.train()\n",
        "      fake_color = model.fake_color.detach()\n",
        "      real_color = model.ab\n",
        "      L = model.L\n",
        "      fake_imgs = lab_to_rgb(L, fake_color)\n",
        "      real_imgs = lab_to_rgb(L, real_color)\n",
        "      fig = plt.figure(figsize=(15, 8))\n",
        "      for i in range(5):\n",
        "          ax = plt.subplot(3, 5, i + 1)\n",
        "          ax.imshow(L[i][0].cpu(), cmap='gray')\n",
        "          ax.axis(\"off\")\n",
        "          ax = plt.subplot(3, 5, i + 1 + 5)\n",
        "          ax.imshow(fake_imgs[i])\n",
        "          ax.axis(\"off\")\n",
        "          ax = plt.subplot(3, 5, i + 1 + 10)\n",
        "          ax.imshow(real_imgs[i])\n",
        "          ax.axis(\"off\")\n",
        "      plt.show()\n",
        "      if save:\n",
        "          fig.savefig(f\"colorization_{time.time()}.png\")\n",
        "          \n",
        "  def log_results(loss_meter_dict):\n",
        "      for loss_name, loss_meter in loss_meter_dict.items():\n",
        "          print(f\"{loss_name}: {loss_meter.avg:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BzjASSt3gAiC"
      },
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "  def train_model(model, train_dl, epochs, display_every=200):\n",
        "      data = next(iter(val_dl)) \n",
        "      for e in range(epochs):\n",
        "          loss_meter_dict = create_loss_meters()\n",
        "          i = 0                                  \n",
        "          for data in tqdm(train_dl):\n",
        "              model.setup_input(data) \n",
        "              model.optimize()\n",
        "              update_losses(model, loss_meter_dict, count=data['L'].size(0)) \n",
        "              i += 1\n",
        "              if i % display_every == 0:\n",
        "                  print(f\"\\nEpoch {e+1}/{epochs}\")\n",
        "                  print(f\"Iteration {i}/{len(train_dl)}\")\n",
        "                  log_results(loss_meter_dict) \n",
        "                  visualize(model, data, save=False) \n",
        "model = MainModel()\n",
        "train_model(model, train_dl, 60)\n",
        "model_save_name = 'coloration.pt'\n",
        "path = F\"/content/drive/MyDrive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
